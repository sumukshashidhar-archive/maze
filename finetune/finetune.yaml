# model and tokenizer
base_model: microsoft/Phi-3-mini-4k-instruct # change for model
sequence_len: 2048

model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
bfloat16: true
pad_to_sequence_len: true
save_safetensors: true

# change for your dataset.
datasets:
  - path: sumuks/maze-problem-multi-size-grid
    type: completion
    train_on_split: train

shuffle_merged_datasets: true

test_datasets:
  - path: sumuks/maze-problem-multi-size-grid
    type: completion
    train_on_split: test

# lora
lora_r: 256
lora_alpha: 32
lora_dropout: 0.05
lora_target_linear: true
lora_modules_to_save:
  - embed_tokens
  - lm_head
use_rslora: true

# logging
wandb_project: maze-runner
wandb_name: sumuk-maze-runner-01

output_dir: ./outputs/sumuk-maze-runner-01

gradient_accumulation_steps: 2
micro_batch_size: 1
eval_batch_size: 1
warmup_ratio: 0.05
learning_rate: 1e-4
lr_scheduler: cosine
optimizer: adamw_torch


evals_per_epoch: 4
saves_per_epoch: 4
eval_table_size: 10
eval_max_new_tokens: 512

deepspeed: finetune/zero1.json